import streamlit as st
from huggingface_hub import InferenceClient

# --- CONFIGURATION ---
st.set_page_config(page_title="Kosher AI", page_icon="ü•Ø")

# --- APP HEADER ---
st.title("üïé The Bracha Bot")
st.subheader("Powered by Llama 3 & Brachos.org Data")
st.caption("Status: SYSTEM ONLINE")

# --- HARDCODED "TRUTH" DATABASE (From Brachos.org) ---
# This ensures 100% accuracy for specific common items
BRACHOS_DB = {
    "pizza": {
        "bracha": "Hamotzi (usually) or Mezonot",
        "explanation": "If you eat 2+ slices or it is a meal, wash and say Hamotzi. For a snack (1 slice), some say Mezonot. (Source: Brachos.org)"
    },
    "apple": {
        "bracha": "Borei Pri Ha-Etz",
        "explanation": "Grows on a tree. Bracha Acharona is Borei Nefashos."
    },
    "banana": {
        "bracha": "Borei Pri Ha-Adamah",
        "explanation": "Even though it looks like a tree, Halacha treats it as a bush because the stem does not stay year-to-year."
    },
    "water": {
        "bracha": "Shehakol",
        "explanation": "The source of all basic liquids."
    },
    "rice": {
        "bracha": "Mezonot",
        "explanation": "Rice is unique; its bracha is Mezonot but its after-bracha is Borei Nefashos."
    }
}

# --- SIDEBAR: API KEY INPUT ---
if "HF_TOKEN" in st.secrets:
    api_key = st.secrets["HF_TOKEN"]
else:
    api_key = st.sidebar.text_input("Paste Hugging Face Token:", type="password")

# --- MAIN APP LOGIC ---
if api_key:
    client = InferenceClient(api_key=api_key)

    food_input = st.text_input("What food are you eating?", placeholder="e.g., A slice of pizza")

    if st.button("Check Bracha"):
        if not food_input:
            st.warning("Please enter a food first!")
        else:
            # Clean the input (make it lowercase for checking the DB)
            clean_food = food_input.lower().strip()
            
            # STEP 1: CHECK THE "TRUTH DATABASE"
            if clean_food in BRACHOS_DB:
                with st.spinner("Consulting Brachos.org Database..."):
                    result = BRACHOS_DB[clean_food]
                    st.success("Match Found in Kosher Database!")
                    st.info(f"**Food:** {clean_food.capitalize()}")
                    st.markdown(f"### **Bracha:** {result['bracha']}")
                    st.write(f"**Halachic Note:** {result['explanation']}")
                    st.caption("Verified by internal database.")
            
            # STEP 2: IF NOT FOUND, ASK THE AI
            else:
                with st.spinner("Consulting the AI Rabbi..."):
                    try:
                        messages = [
                            {
                                "role": "user", 
                                "content": f"I am eating {food_input}. 1. Identify the food. 2. State the correct Bracha Rishona. 3. Briefly explain why based on standard Orthodox Halacha."
                            }
                        ]
                        
                        # Using Llama 3 (Conversational)
                        response = client.chat_completion(
                            model="meta-llama/Meta-Llama-3-8B-Instruct", 
                            messages=messages, 
                            max_tokens=500
                        )
                        
                        answer = response.choices[0].message.content
                        
                        st.success("Analysis Complete (AI Generated)")
                        st.write(answer)
                        st.warning("‚ö†Ô∏è Result generated by AI. Verify with a Rabbi or Brachos.org")
                        
                    except Exception as e:
                        st.error(f"Error: {e}")
                        st.info("Tip: If the server is busy, wait 30 seconds and try again.")
else:
    st.warning("üëà System needs a Token. Check your Secrets or Sidebar.")
